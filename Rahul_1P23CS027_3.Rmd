---
output:
  
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
        
  pdf_document:
    
    fig_caption: true
    fig_crop: false
  word_document: default
params:
    printcode: false
---
---
title: "MACHINE LEARNING ASSIGNMENT-3"
author: "RAHUL_R"
date: "2024-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(ISLR)

```

```{r}
library(MASS)
```

```{r}
library(class)
```


> In this problem, you will develop a model to predict whether a given car gets
> high or low gas mileage based on the `Auto` data set.
>
> a. Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a
>    value above its median, and a 0 if `mpg` contains a value below its
>    median. You can compute the median using the `median()` function. Note you
>    may find it helpful to use the `data.frame()` function to create a single
>    data set containing both `mpg01` and the other `Auto` variables.

```{r}
(med <- median(Auto$mpg))
mpg01 <- ifelse(Auto$mpg>med,1,0)
x <- data.frame(Auto,mpg01)
```

> b. Explore the data graphically in order to investigate the association
>    between `mpg01` and the other features. Which of the other features seem
>    most likely to be useful in predicting `mpg01`? Scatterplots and boxplots
>    may be useful tools to answer this question. Describe your findings.


```{r}
cor(x[,sapply(x, is.numeric)])
```

```{r}
plot(x)
attach(x)
```
```{r}
par(mfrow=c(2,2))
boxplot(cylinders~mpg01,data = x,col=8)
boxplot(displacement~mpg01,data = x,col=2)
boxplot(horsepower~mpg01,data = x,col=3)
boxplot(weight~mpg01,data = x,col=5)
```
- Most variables show an association with `mpg01` category, and several variables are colinear.

> c. Split the data into a training set and a test set.

```{r}
set.seed(1)
train_mpg <- sample(seq_len(nrow(x)), nrow(x) / 2)
training_data_mpg <- x[train_mpg,]
testing_data_mpg <- x[-train_mpg,]
testing_mpg01 <- x[-train_mpg,]$mpg01

```


> d. Perform LDA on the training data in order to predict `mpg01` using the
>    variables that seemed most associated with `mpg01` in (b). What is the
>    test error of the model obtained?

```{r}
q6_dfit <- lda(mpg01 ~ cylinders + weight + displacement, data = training_data_mpg)
pred <- predict(q6_dfit, testing_data_mpg, type = "response")$class
(t <- table(pred,testing_mpg01))
sum(diag(t)) / sum(t)
mean(pred != testing_mpg01)
```
The overall fraction of error in predictions is 0.1173469.
The error rate is 0.1173469.
The model mainly predicts the majority class ("0") but misclassifies many "1" instances.

> e. Perform QDA on the training data in order to predict `mpg01` using the
>    variables that seemed most associated with `mpg01` in (b). What is the
>    test error of the model obtained?

```{r}
q6_efit <- qda(mpg01 ~ cylinders + weight + displacement, data = training_data_mpg)
pred <- predict(q6_efit, testing_data_mpg, type = "response")$class
(t <- table(pred,testing_mpg01))
sum(diag(t)) / sum(t)
mean(pred != testing_mpg01)
```
The overall fraction of error in predictions is 0.1122449.
The error rate is 0.1122449
The model mainly predicts "1" instances well but misclassifies some "0" instances as "1"

> f. Perform logistic regression on the training data in order to predict
>    `mpg01` using the variables that seemed most associated with `mpg01` in
>    (b). What is the test error of the model obtained?

```{r}
fit <- glm(mpg01 ~ cylinders + weight + displacement, data = training_data_mpg, family = binomial)
pred <- predict(fit, testing_data_mpg, type = "response") > 0.5
(t <- table(pred,testing_mpg01))
sum(diag(t)) / sum(t)
mean(pred != testing_mpg01)
```
The overall fraction of correct predictions is 0.877551.
The error rate is 0.122449.
The model identifies most "1" instances well but misclassifies some "0" instances as "1" 

> g. Perform KNN on the training data, with several values of $K$, in order to
>    predict `mpg01`. Use only the variables that seemed most associated with
>    `mpg01` in (b). What test errors do you obtain? Which value of $K$ seems
>    to perform the best on this data set?

```{r}
res <- sapply(1:50, function(k) {
  fit <- knn(training_data_mpg[ c(2,5,3)], testing_data_mpg[ c(2,5,3)], testing_mpg01, k = k)
  mean(fit != testing_mpg01)
})
names(res) <- 1:50
res[which.min(res)]
plot(res, type = "o")
```

For the models tested here, $k = 34$ appears to perform best. QDA has a lower error rate overall, performing slightly better than LDA.
